import streamlit as st
import os
import datetime
import numpy as np
import matplotlib.pyplot as plt
from src.models import StudentSession, TestResult, TagInstance
from src.utils import PARTS, PASSAGGIO_CRITERIA, TESTS
from src.audio_processor import AudioProcessor
from src.pdf_generator import PDFGenerator

# Constants
RECORDINGS_DIR = "recordings"
REPORTS_DIR = "reports"
os.makedirs(RECORDINGS_DIR, exist_ok=True)
os.makedirs(REPORTS_DIR, exist_ok=True)

# Initialize Session State
if 'session' not in st.session_state:
    st.session_state['session'] = StudentSession()
if 'current_test_index' not in st.session_state:
    st.session_state['current_test_index'] = 0

def save_uploaded_file(uploaded_file, test_id):
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{st.session_state['session'].id}_{test_id}_{timestamp}.wav"
    filepath = os.path.join(RECORDINGS_DIR, filename)
    with open(filepath, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return filepath

def generate_diagnosis(result, part):
    """Generates meaningful diagnostic text based on metrics."""
    diagnosis = []
    
    # Accuracy
    if result.pitch_accuracy_cents >= 900.0:
        diagnosis.append("ìŒì •ì´ ê°ì§€ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (ì†ŒìŒ/ë¬´ìŒ)")
    elif result.pitch_accuracy_cents > 50:
        diagnosis.append(f"í”¼ì¹˜ê°€ ëª©í‘œìŒë³´ë‹¤ í‰ê·  {result.pitch_accuracy_cents:.1f} cents ë²—ì–´ë‚¬ìŠµë‹ˆë‹¤. (ì •í™•ë„ ì£¼ì˜)")
    elif result.pitch_accuracy_cents < 20:
        diagnosis.append("í”¼ì¹˜ ì •í™•ë„ê°€ ë§¤ìš° ìš°ìˆ˜í•©ë‹ˆë‹¤.")

    # Stability
    if result.pitch_stability_cents >= 900.0:
        pass # Already handled by accuracy failure message
    elif result.pitch_stability_cents > 30:
        diagnosis.append("ìŒì˜ í”ë“¤ë¦¼(Vibrato/Tremolo)ì´ ë‹¤ì†Œ í½ë‹ˆë‹¤. í˜¸í¡ ì§€íƒ±ì„ í™•ì¸í•˜ì„¸ìš”.")
    elif result.pitch_stability_cents < 10 and result.pitch_stability_cents >= 0:
        diagnosis.append("ìŒì´ ë§¤ìš° ì•ˆì •ì ì…ë‹ˆë‹¤ (Straight Tone).")

    # Drift
    if result.pitch_drift_cents < -20:
        diagnosis.append("ëìŒì´ ì²˜ì§€ëŠ” ê²½í–¥(Flat Drift)ì´ ìˆìŠµë‹ˆë‹¤.")
    elif result.pitch_drift_cents > 20:
        diagnosis.append("ëìŒì´ ìƒµë˜ëŠ” ê²½í–¥(Sharp Drift)ì´ ìˆìŠµë‹ˆë‹¤.")

    # Voiced Ratio
    # We need to add voiced_ratio to TestResult model first, or just use it here if returned
    # Assuming result object has it or we add it to the model.
    # For now, simplistic check if accuracy is 0 (which might imply no voiced frames found)
    
    # New Strict Check: On-Target Ratio
    if result.pitch_on_target_ratio < 0.6:
        diagnosis.append("ìŒì •ì´ ë¶ˆì•ˆì •í•˜ì—¬ ëª©í‘œìŒì„ ë§ì´ ë²—ì–´ë‚©ë‹ˆë‹¤. (ì •í™•ë„ < 60%)")
    elif result.pitch_on_target_ratio < 0.85:
        diagnosis.append("ì¤‘ê°„ì¤‘ê°„ ìŒì •ì´ í”ë“¤ë¦½ë‹ˆë‹¤. í˜¸í¡ ì§€íƒ±ì— ì‹ ê²½ì“°ì„¸ìš”.")
    
    if not diagnosis:
        diagnosis.append("ì „ë°˜ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ë°œì„±ì…ë‹ˆë‹¤. ì„¸ë¶€ ì§€í‘œì¸ ë¹„ë¸Œë¼í†  ì†ë„ ë“±ì„ ì²´í¬í•´ë³´ì„¸ìš”.")

    return " ".join(diagnosis)

def analyze_audio(filepath, test_id, target_note=None, voice_part="Soprano"):
    processor = AudioProcessor()
    y, sr = processor.load_audio(filepath)
    times, f0, rms, voiced_probs = processor.extract_features(y)
    
    # Calculate Metrics with new strict logic
    metrics = processor.calculate_metrics(f0, rms, voiced_probs, target_note, voice_part)
    
    # Validation for Test 6 (After) vs Test 1 (Before)
    if test_id == "T6":
         t1_result = st.session_state['session'].get_result("T1")
         if t1_result:
             # Check duration similarity? 
             duration = times[-1] if len(times) > 0 else 0
             t1_duration = t1_result.pitch_track_time[-1] if t1_result.pitch_track_time else 0
             if abs(duration - t1_duration) > 2.0:
                 st.warning(f"âš ï¸ Warning: Test 6 duration ({duration:.1f}s) differs significantly from Test 1 ({t1_duration:.1f}s). Comparability may be low.")

    result = TestResult(
        test_id=test_id,
        test_name=TESTS[st.session_state['current_test_index']]['name'],
        audio_file_path=filepath,
        pitch_track_time=times.tolist(),
        pitch_track_hz=f0.tolist(), # Contains nans now
        energy_track_time=times.tolist(),
        energy_track_rms=rms.tolist(),
        pitch_accuracy_cents=metrics['accuracy'],
        pitch_stability_cents=metrics['stability'],
        pitch_drift_cents=metrics['drift'],
        pitch_on_target_ratio=metrics.get('on_target_ratio', 0.0),
        attack_overshoot_score=metrics['overshoot'],
        processed_at=datetime.datetime.now()
    )
    
    # Generate and attach diagnosis tag
    diag_text = generate_diagnosis(result, voice_part)
    result.tags.append(TagInstance(tag_type="Diagnosis", description=diag_text))
    
    return result

def main():
    st.set_page_config(page_title="Vocal Diagnostic Report", layout="wide")
    st.title("ğŸ¤ Vocal Diagnostic Report (VDR)")

    # Sidebar
    with st.sidebar:
        st.title("VDR Settings")
        
        # Version & Environment Check
        import sys
        is_python_3_13 = sys.version_info >= (3, 13)
        
        if is_python_3_13:
            st.error("âš ï¸ CRITICAL UPDATE REQUIRED")
            st.markdown("""
            **í˜„ì¬ Python 3.13 (í˜¸í™˜ ë¶ˆê°€) ì‹¤í–‰ ì¤‘!**
            
            ë¦¬ë¶€íŒ…/ì‚­ì œ ë²„íŠ¼ì´ ì•ˆ ë³´ì´ì‹ ë‹¤ë©´, ì•„ë˜ **ë¹¨ê°„ ë²„íŠ¼**ì„ ëˆŒëŸ¬ë³´ì„¸ìš”.
            ì„œë²„ë¥¼ ê°•ì œë¡œ ì¢…ë£Œì‹œì¼œì„œ ì¬ë¶€íŒ…ì„ ìœ ë„í•©ë‹ˆë‹¤.
            """)
            
            if st.button("ğŸ’£ FORCE SERVER REBOOT (Emergency)", type="primary"):
                st.warning("Killing server process... Please wait for automatic restart.")
                import os
                import signal
                import time
                time.sleep(1)
                os.kill(os.getpid(), signal.SIGKILL)
            
            st.caption(f"Current: Python {sys.version.split()[0]} âŒ")
        else:
            st.caption(f"v1.11 (Python {sys.version.split()[0]} OK) âœ…")
        
        # User Profile
        st.subheader("Student Profile")
        st.session_state['session'].student_name = st.text_input("Name", st.session_state['session'].student_name)
        st.session_state['session'].part = st.selectbox("Part", PARTS, index=PARTS.index(st.session_state['session'].part))
        st.session_state['session'].coach_name = st.text_input("Coach", st.session_state['session'].coach_name)
        
        # Update Passaggio Info
        st.session_state['session'].passaggio_info = PASSAGGIO_CRITERIA[st.session_state['session'].part]
        st.info(f"Passaggio: {st.session_state['session'].passaggio_info['desc']}")
        
        # Debug Info
        with st.expander("ğŸ› ï¸ Debug Info (Show to Developer)", expanded=True):
            import sys
            import subprocess
            
            st.code(f"Python: {sys.version.split()[0]}")
            
            # Show pip freeze to debug installation
            try:
                result = subprocess.run([sys.executable, '-m', 'pip', 'freeze'], capture_output=True, text=True)
                installed_packages = result.stdout
                st.text("Installed Packages:")
                st.code(installed_packages, language="text", line_numbers=True)
            except Exception as e:
                st.error(f"Failed to list packages: {e}")

            # Specific Checks
            try:
                import mediapipe as mp
                st.success(f"MediaPipe: {mp.__version__}")
                st.write(f"Has solutions? {'âœ…' if hasattr(mp, 'solutions') else 'âŒ'}")
            except ImportError:
                st.error("MediaPipe: Not Installed")
                
            try:
                import cv2
                st.success(f"OpenCV: {cv2.__version__}")
            except ImportError:
                st.error("OpenCV: Not Installed")
        
        st.markdown("---")
        
        # MIDI Part Player
        st.subheader("ğŸ¹ MIDI Part Practice")
        midi_file = st.file_uploader("Upload MIDI File", type=["mid", "midi"])
        
        if midi_file:
            from src.midi_handler import get_midi_tracks, synthesis_midi_track
            # Read file pointer compatible with pretty_midi
            # pretty_midi expects file path or file-like object
            tracks, midi_data = get_midi_tracks(midi_file)
            
            if tracks:
                track_names = [f"{t['index']}: {t['name']}" for t in tracks]
                selected_track_str = st.selectbox("Select Part to Play", track_names)
                selected_index = int(selected_track_str.split(":")[0])
                
                if st.button("Generate & Play Part"):
                    with st.spinner("Synthesizing Audio..."):
                        wav_bytes = synthesis_midi_track(midi_data, selected_index)
                        if wav_bytes:
                            st.audio(wav_bytes, format='audio/wav')
                        else:
                            st.error("Failed to synthesize track.")
            else:
                st.error("No tracks found or invalid MIDI.")
        
        st.markdown("---")

        if st.button("Reset Session"):
            st.session_state['session'] = StudentSession()
            st.session_state['current_test_index'] = 0
            st.rerun()

    # Main Area: Test Flow
    tests = TESTS
    current_index = st.session_state['current_test_index']
    
    if current_index < len(tests):
        test = tests[current_index]
        st.subheader(f"Test {current_index + 1}/{len(tests)}: {test['name']}")

    # Tabs for Audio / Video
    tab1, tab2 = st.tabs(["ğŸ¤ Audio Analysis", "ğŸ“¹ Video Analysis (Face Tension)"])

    with tab1:
        st.markdown(f"**Instructions**: {test['description']}")
        
        # Display Target Note for Sustained Tests
        if "Sustained" in test['name']:
            target_map = {
                "Soprano": "F5 (698 Hz)", 
                "Alto": "E5 (659 Hz)", 
                "Tenor": "F4 (349 Hz)", 
                "Baritone": "E4 (330 Hz)", 
                "Bass": "Eb4 (311 Hz)"
            }
            target_note = target_map.get(st.session_state['session'].part, "C4").split(" ")[0] # Extract "F4" from "F4 (349 Hz)"
            st.info(f"ğŸµ **Target Note (Passaggio Start)**: {target_map.get(st.session_state['session'].part)}")
            
            # Play Reference Pitch
            try:
                # Extract Hz from string "F4 (349 Hz)" -> 349
                target_str = target_map.get(st.session_state['session'].part, "C4 (261 Hz)")
                hz_str = target_str.split("(")[1].split(" ")[0]
                target_hz = float(hz_str)
                
                # Use Piano Synth for Reference Pitch
                from src.synth import generate_piano_note
                import soundfile as sf
                import io
                import numpy as np
                
                # Generate 2 seconds of Piano C4/F4/etc
                waveform = generate_piano_note(target_hz, duration=2.0)
                # Normalize
                waveform = waveform / np.max(np.abs(waveform)) if np.max(np.abs(waveform)) > 0 else waveform
                
                buf = io.BytesIO()
                sf.write(buf, waveform, 44100, format='WAV', subtype='PCM_16')
                buf.seek(0) 
                tone_bytes = buf
                
                st.audio(tone_bytes, format='audio/wav', start_time=0)
                st.caption("ğŸ¹ Play Reference Pitch (Piano)")
            except Exception as e:
                st.warning(f"Audio Playback Error: {e}")
        
        st.markdown(f"_Duration Guide: {test['duration_guide']} seconds_")
        
        # Recording / Upload
        audio_value = st.audio_input(f"Record {test['name']}")
        
        if audio_value:
            st.audio(audio_value, format='audio/wav')
            
            with st.spinner("Analyzing..."):
                # Save to temp file
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                    tmp_file.write(audio_value.read())
                    tmp_path = tmp_file.name
                
                # Analyze
                from src.audio_processor import AudioProcessor
                processor = AudioProcessor()
                y, sr = processor.load_audio(tmp_path)
                metrics = processor.calculate_metrics(y, sr, target_note=test.get('target_note'))
                
                st.success("Analysis Complete!")
                
                # Diagnosis
                diagnosis = generate_diagnosis(metrics, test['name'])
                st.session_state['session'].add_result(
                    test_name=test['name'],
                    metrics=metrics,
                    diagnosis=diagnosis
                )
                
                # Display Result
                col1, col2, col3 = st.columns(3)
                col1.metric("Pitch Accuracy (Cents)", f"{metrics['accuracy']:.1f}", delta_color="inverse")
                col2.metric("Stability (Std Dev)", f"{metrics['stability']:.1f}", delta_color="inverse")
                col3.metric("Drift (Slope)", f"{metrics['drift']:.2f}")
                
                 # Feedback based on On-Target Ratio
                if metrics['on_target_ratio'] < 0.6:
                    st.warning(f"âš ï¸ **Unstable Pitch**: Only {metrics['on_target_ratio']*100:.1f}% of frames were on target.")
                else:
                    st.success(f"âœ… **Stable Pitch**: {metrics['on_target_ratio']*100:.1f}% on target.")

                # Diagnosis List
                st.write("### ğŸ©º Diagnosis")
                for item in diagnosis:
                    st.write(f"- {item}")
                
                # Graphs
                st.plotly_chart(generate_pitch_plot(y, sr, metrics['mean_pitch_hz']), use_container_width=True)

    with tab2:
        st.markdown("### ğŸ“¹ Facial Tension Analysis")
        st.info("ğŸ’¡ **On Mobile?** Tap 'Browse files' -> Select 'Camera' to record directly!\n\n(Desktop browsers may only support file upload for now.)")
        
        video_file = st.file_uploader("Upload Video (or Record on Mobile)", type=["mp4", "mov", "avi"])
        
        if video_file:
            # Display Video
            st.video(video_file)
            
            if st.button("Analyze Face Tension"):
                with st.spinner("Processing Video (MediaPipe Face Mesh)..."):
                    try:
                        import tempfile
                        # Create temp file, write, and CLOSE it so other libs can read it
                        tfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') 
                        tfile.write(video_file.read())
                        tfile.close() # Critical: Close before OpenCV opens it
                        
                        from src.video_processor import VideoProcessor
                        vp = VideoProcessor()
                        df = vp.process_video(tfile.name)
                        
                        # Cleanup
                        os.unlink(tfile.name)
                        
                        if df is not None and not df.empty:
                            st.success("Analysis Complete!")
                            fig = vp.generate_tension_chart(df)
                            st.plotly_chart(fig, use_container_width=True)
                            
                            avg_ratio = df['tension_ratio'].mean()
                            st.metric("Average Tension Ratio (Width/Height)", f"{avg_ratio:.2f}")
                            
                            if avg_ratio > 1.5:
                                st.warning("âš ï¸ **High Horizontal Tension**: Your mouth tends to spread wide (smile shape). Try to drop your jaw more for a vertical vowel shape.")
                            else:
                                st.success("âœ… **Good Jaw Opening**: Your mouth shape seems balanced.")
                                
                        else:
                            st.error("Could not detect face/landmarks in the video. Please ensure face is visible.")
                            
                    except Exception as e:
                        import traceback
                        st.error(f"Video Processing Error: {e}")
                        st.code(traceback.format_exc())

    st.markdown("---")
    if st.button("Next Test ->"):
        st.session_state['current_test_index'] += 1
        st.rerun()
                
    else:
        # All Tests Completed
        st.header("ğŸ‰ Diagnosis Complete!")
        st.success("All tests recorded and analyzed.")
        
        st.subheader("Coach's Final Comment")
        st.session_state['session'].coach_comment = st.text_area("Diagnosis & Observations", st.session_state['session'].coach_comment)
        st.session_state['session'].routine_assignment = st.text_area("Prescribed Routine", st.session_state['session'].routine_assignment)
        
        if st.button("Generate PDF Report"):
            try:
                gen = PDFGenerator()
                filename = f"VDR_Report_{st.session_state['session'].student_name}_{datetime.datetime.now().strftime('%Y%m%d')}.pdf"
                filepath = os.path.join(REPORTS_DIR, filename)
                gen.generate_report(st.session_state['session'], filepath)
                
                with open(filepath, "rb") as f:
                    st.download_button("Download PDF Report", f, file_name=filename, mime="application/pdf")
                st.success(f"Report Generated: {filename}")
            except Exception as e:
                st.error(f"Failed to generate report: {e}")
                st.exception(e)

if __name__ == "__main__":
    main()
